#!/usr/bin/env python3.12
"""
GSC 인덱싱 상태 주간 리포트 생성기

GSC "Why pages aren't indexed" 데이터를 Playwright로 스크래핑하여
주간 변화 추적 리포트를 자동 생성합니다.

사용법:
    gsc-index-report --login              # 최초 쿠키 설정 (브라우저 열림)
    gsc-index-report --headless           # headless 모드 실행
    gsc-index-report --site querypie.com  # 특정 사이트만 실행
    gsc-index-report --no-compare         # 비교 없이 현재 스냅샷만
"""

import argparse
import asyncio
import json
import os
import sys
import urllib.parse
from datetime import datetime
from pathlib import Path

try:
    from playwright.async_api import async_playwright
except ImportError:
    print("Playwright가 설치되어 있지 않습니다:")
    print("  pip install playwright && playwright install chromium")
    sys.exit(1)


# --- 설정 ---

COOKIE_PATH = Path(os.environ.get(
    'GSC_COOKIE_PATH',
    os.path.expanduser('~/.config/gsc/browser-cookies.json')
))

REMEDIATION = {
    'Crawled - currently not indexed':
        '콘텐츠 품질 개선, 내부 링크 보강, thin content 제거',
    'Discovered - currently not indexed':
        '사이트맵 제출 확인, 크롤 예산 최적화, 불필요 URL 정리',
    "Excluded by 'noindex' tag":
        '의도된 설정인지 확인, 의도하지 않았다면 태그 제거',
    'Blocked by robots.txt':
        'robots.txt 규칙 검토, 필요한 페이지 허용',
    'Not found (404)':
        '301 리다이렉트 설정 또는 사이트맵에서 제거',
    'Soft 404':
        '실제 콘텐츠 추가 또는 적절한 404 응답 반환',
    'Redirect page':
        '사이트맵에서 리다이렉트 URL 제거, 최종 URL만 포함',
    'Duplicate without user-selected canonical':
        'canonical 태그 설정 또는 중복 페이지 통합',
    'Duplicate, Google chose different canonical than user':
        'canonical 태그와 실제 콘텐츠 일치 확인, 중복 콘텐츠 정리',
    'Page with redirect':
        '리다이렉트 체인 정리, 사이트맵에서 최종 URL만 포함',
    'Server error (5xx)':
        '서버 오류 원인 파악 및 수정, 서버 안정성 확인',
    'Blocked due to unauthorized request (401)':
        '인증 설정 확인, 크롤러 접근 허용',
    'Blocked due to access forbidden (403)':
        '접근 권한 설정 확인, 크롤러 IP/UA 허용',
}
REMEDIATION_FALLBACK = '확인 필요: GSC에서 상세 내용 직접 확인'


# --- 함수 ---

def load_config(config_path: str) -> dict:
    """설정 파일을 로드합니다."""
    path = Path(config_path)
    if not path.exists():
        print(f"오류: 설정 파일을 찾을 수 없습니다: {config_path}")
        sys.exit(1)
    with open(path, encoding='utf-8') as f:
        return json.load(f)


async def login_and_save_cookies():
    """브라우저를 열어 GSC 로그인 후 쿠키를 저장합니다."""
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        context = await browser.new_context()
        page = await context.new_page()
        await page.goto('https://search.google.com/search-console')
        print("브라우저에서 Google 계정으로 로그인하세요.")
        input("로그인 후 Enter를 눌러주세요...")
        cookies = await context.cookies()
        COOKIE_PATH.parent.mkdir(parents=True, exist_ok=True)
        with open(COOKIE_PATH, 'w', encoding='utf-8') as f:
            json.dump(cookies, f, ensure_ascii=False, indent=2)
        print(f"쿠키 저장 완료: {COOKIE_PATH}")
        await browser.close()


async def load_cookies(context):
    """저장된 쿠키를 브라우저 컨텍스트에 로드합니다."""
    if not COOKIE_PATH.exists():
        print(f"오류: 쿠키 파일이 없습니다: {COOKIE_PATH}")
        print("먼저 --login으로 로그인하세요.")
        sys.exit(1)
    with open(COOKIE_PATH, encoding='utf-8') as f:
        cookies = json.load(f)
    await context.add_cookies(cookies)


def build_gsc_page_indexing_url(site_url: str) -> str:
    """GSC Page Indexing 리포트 URL 생성."""
    encoded = urllib.parse.quote(site_url, safe='')
    return f"https://search.google.com/search-console/index?resource_id={encoded}"


async def scrape_site(page, site_url: str, site_label: str) -> dict:
    """GSC에서 사이트의 인덱싱 상태를 스크래핑합니다."""
    url = build_gsc_page_indexing_url(site_url)
    await page.goto(url, wait_until='networkidle')
    await page.wait_for_selector('table', timeout=30000)
    await page.wait_for_timeout(3000)

    # 첫 번째 테이블 = "Why pages aren't indexed"
    tables = await page.query_selector_all('table')
    if not tables:
        raise RuntimeError(f'{site_label}: 테이블을 찾을 수 없습니다')
    table = tables[0]

    # 헤더 행 제외, 데이터 행만 파싱
    # 컬럼: Reason(0), Source(1), Validation(2), Trend(3), Pages(4)
    rows = await table.query_selector_all('tr')
    reasons = []
    for row in rows[1:]:  # 헤더 행 건너뛰기
        cells = await row.query_selector_all('td')
        if len(cells) >= 5:
            reason = (await cells[0].inner_text()).strip()
            source = (await cells[1].inner_text()).strip()
            validation_raw = (await cells[2].inner_text()).strip()
            # validation 컬럼에 아이콘 문자가 섞여 있으므로 정리
            validation = validation_raw.split('\n')[-1].strip() if '\n' in validation_raw else validation_raw
            pages_text = (await cells[4].inner_text()).strip()
            pages_count = int(pages_text.replace(',', '')) if pages_text.replace(',', '').isdigit() else 0
            reasons.append({
                'reason': reason,
                'source': source,
                'validation': validation,
                'pages': pages_count,
            })

    return {
        'site_url': site_url,
        'label': site_label,
        'reasons': reasons,
        'scraped_at': datetime.now().isoformat(),
    }


async def scrape_site_with_retry(page, site_url: str, site_label: str, max_retries: int = 2) -> dict:
    """재시도 로직이 포함된 스크래핑."""
    for attempt in range(max_retries + 1):
        try:
            return await scrape_site(page, site_url, site_label)
        except Exception as e:
            if attempt == max_retries:
                return {
                    'site_url': site_url,
                    'label': site_label,
                    'error': str(e),
                    'scraped_at': datetime.now().isoformat(),
                }
            print(f"  재시도 {attempt + 1}/{max_retries}: {e}")
            await page.wait_for_timeout(3000)


def save_data(data: dict, output_dir: str) -> Path:
    """스크래핑 데이터를 JSON으로 저장합니다."""
    data_dir = Path(output_dir) / 'data'
    data_dir.mkdir(parents=True, exist_ok=True)
    date_str = datetime.now().strftime('%Y-%m-%d')
    path = data_dir / f'gsc-index-{date_str}.json'
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    return path


def find_previous_data(output_dir: str) -> dict | None:
    """이전 주 데이터를 로드합니다."""
    data_dir = Path(output_dir) / 'data'
    files = sorted(data_dir.glob('gsc-index-*.json'), reverse=True)
    today = datetime.now().strftime('%Y-%m-%d')
    for f in files:
        if today not in f.name:
            with open(f, encoding='utf-8') as fh:
                return json.load(fh)
    return None


def compare_with_previous(current_data: dict, previous_data: dict | None) -> dict:
    """현재 데이터와 이전 데이터를 비교합니다."""
    if previous_data is None:
        return {'has_previous': False}

    comparison = {'has_previous': True, 'sites': {}}
    prev_by_label = {s['label']: s for s in previous_data.get('sites', [])}

    for site in current_data.get('sites', []):
        label = site['label']
        prev_site = prev_by_label.get(label)
        if not prev_site or 'error' in site:
            comparison['sites'][label] = {'new_site': True}
            continue

        prev_reasons = {r['reason']: r['pages'] for r in prev_site.get('reasons', [])}
        curr_reasons = {r['reason']: r['pages'] for r in site.get('reasons', [])}

        changes = {}
        all_reasons = set(prev_reasons) | set(curr_reasons)
        for reason in all_reasons:
            prev_count = prev_reasons.get(reason, 0)
            curr_count = curr_reasons.get(reason, 0)
            diff = curr_count - prev_count
            is_new = reason not in prev_reasons
            is_resolved = reason not in curr_reasons
            changes[reason] = {
                'previous': prev_count,
                'current': curr_count,
                'diff': diff,
                'is_new': is_new,
                'is_resolved': is_resolved,
            }
        comparison['sites'][label] = changes

    return comparison


def generate_report(current_data: dict, comparison: dict, output_dir: str) -> Path:
    """Markdown 리포트를 생성합니다."""
    today = datetime.now().strftime('%Y-%m-%d')
    now = datetime.now().strftime('%Y-%m-%d %H:%M')
    has_prev = comparison.get('has_previous', False)

    lines = []
    lines.append('# GSC 인덱싱 상태 주간 리포트\n')
    lines.append(f'- 생성: {now} (자동)')
    if has_prev:
        lines.append('- 전주 데이터와 비교 포함')
    lines.append('')

    # 전체 요약 테이블
    lines.append('## 전체 요약\n')
    if has_prev:
        lines.append('| 사이트 | 미인덱싱 페이지 | 전주 대비 | 신규 문제 | 해결됨 |')
        lines.append('|--------|----------------|----------|----------|--------|')
    else:
        lines.append('| 사이트 | 미인덱싱 페이지 |')
        lines.append('|--------|----------------|')

    total_pages = 0
    total_diff = 0
    total_new = 0
    total_resolved = 0
    alerts = []

    for site in current_data.get('sites', []):
        label = site['label']
        if 'error' in site:
            if has_prev:
                lines.append(f'| {label} | 오류: {site["error"]} | - | - | - |')
            else:
                lines.append(f'| {label} | 오류: {site["error"]} |')
            continue

        site_pages = sum(r['pages'] for r in site.get('reasons', []))
        total_pages += site_pages

        if has_prev:
            site_changes = comparison.get('sites', {}).get(label, {})
            if site_changes.get('new_site'):
                lines.append(f'| {label} | {site_pages} | 신규 | - | - |')
            else:
                site_diff = sum(c['diff'] for c in site_changes.values())
                site_new = sum(1 for c in site_changes.values() if c['is_new'])
                site_resolved = sum(1 for c in site_changes.values() if c['is_resolved'])
                total_diff += site_diff
                total_new += site_new
                total_resolved += site_resolved
                diff_str = f'+{site_diff}' if site_diff > 0 else str(site_diff)
                lines.append(f'| {label} | {site_pages} | {diff_str} | {site_new} | {site_resolved} |')

                for reason, change in site_changes.items():
                    if change['diff'] >= 5:
                        alerts.append(
                            f'- {label}: "{reason}" {change["diff"]:+d} '
                            f'({change["previous"]}→{change["current"]})')
                    if change['is_new']:
                        alerts.append(
                            f'- {label}: "{reason}" 신규 발생 ({change["current"]}페이지)')
        else:
            lines.append(f'| {label} | {site_pages} |')

    if has_prev:
        diff_str = f'+{total_diff}' if total_diff > 0 else str(total_diff)
        lines.append(f'| **합계** | **{total_pages}** | **{diff_str}** | **{total_new}** | **{total_resolved}** |')
    else:
        lines.append(f'| **합계** | **{total_pages}** |')
    lines.append('')

    # 주의 필요 항목
    if alerts:
        lines.append('## 주의 필요 항목\n')
        lines.extend(alerts)
        lines.append('')

    # 사이트별 상세
    lines.append('## 사이트별 상세\n')
    for site in current_data.get('sites', []):
        label = site['label']
        lines.append(f'### {label}\n')
        if 'error' in site:
            lines.append(f'스크래핑 오류: {site["error"]}\n')
            continue

        site_changes = comparison.get('sites', {}).get(label, {}) if has_prev else {}

        if has_prev and not site_changes.get('new_site'):
            lines.append('| Reason | Pages | 변화 | Source | Validation |')
            lines.append('|--------|-------|------|--------|------------|')
        else:
            lines.append('| Reason | Pages | Source | Validation |')
            lines.append('|--------|-------|--------|------------|')

        action_items = []
        for r in site.get('reasons', []):
            reason = r['reason']
            pages = r['pages']
            source = r.get('source', '')
            validation = r.get('validation', '')

            if has_prev and not site_changes.get('new_site') and reason in site_changes:
                change = site_changes[reason]
                diff = change['diff']
                if change['is_new']:
                    diff_str = '신규'
                elif diff > 0:
                    diff_str = f'+{diff}'
                elif diff < 0:
                    diff_str = str(diff)
                else:
                    diff_str = '0'
                lines.append(f'| {reason} | {pages} | {diff_str} | {source} | {validation} |')
            else:
                if has_prev and not site_changes.get('new_site'):
                    lines.append(f'| {reason} | {pages} | - | {source} | {validation} |')
                else:
                    lines.append(f'| {reason} | {pages} | {source} | {validation} |')

            remediation = REMEDIATION.get(reason, REMEDIATION_FALLBACK)
            if has_prev and reason in site_changes:
                change = site_changes[reason]
                if change['is_new']:
                    action_items.append(f'- {reason} (신규): {remediation}')
                elif change['diff'] > 0:
                    action_items.append(f'- {reason} ({change["diff"]:+d}): {remediation}')
            elif not has_prev:
                action_items.append(f'- {reason}: {remediation}')

        if action_items:
            lines.append('')
            lines.append('**권장 조치:**')
            lines.extend(action_items)
        lines.append('')

    # 파일 저장
    report_path = Path(output_dir) / f'gsc-index-report-{today}.md'
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(lines))
    return report_path


async def run(args):
    """메인 실행 흐름."""
    if args.login:
        await login_and_save_cookies()
        return

    config = load_config(args.config)
    sites = config['sites']
    if args.site:
        sites = [s for s in sites if s['label'] == args.site]
        if not sites:
            print(f"오류: '{args.site}' 사이트를 찾을 수 없습니다.")
            sys.exit(1)

    print(f"대상 사이트: {', '.join(s['label'] for s in sites)}")

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=args.headless)
        context = await browser.new_context()
        await load_cookies(context)
        page = await context.new_page()

        results = {'sites': [], 'generated_at': datetime.now().isoformat()}
        for site in sites:
            print(f"스크래핑 중: {site['label']}...")
            data = await scrape_site_with_retry(page, site['url'], site['label'])
            results['sites'].append(data)
            if 'error' in data:
                print(f"  오류: {data['error']}")
            else:
                count = len(data.get('reasons', []))
                print(f"  완료: {count}개 사유 수집")

        await browser.close()

    data_path = save_data(results, args.output_dir)
    print(f"데이터 저장: {data_path}")

    previous = None if args.no_compare else find_previous_data(args.output_dir)
    comparison = compare_with_previous(results, previous)
    report_path = generate_report(results, comparison, args.output_dir)
    print(f"리포트 생성: {report_path}")


def main():
    script_dir = Path(__file__).resolve().parent.parent
    default_config = str(script_dir / 'config' / 'gsc-sites.json')
    default_output = str(script_dir / 'reports')

    parser = argparse.ArgumentParser(
        description='GSC 인덱싱 상태 주간 리포트 생성기',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
예시:
  gsc-index-report --login              # 최초 쿠키 설정
  gsc-index-report --headless           # headless 모드 실행
  gsc-index-report --site querypie.com  # 특정 사이트만
  gsc-index-report --no-compare         # 비교 없이 스냅샷만
"""
    )
    parser.add_argument('--login', action='store_true',
                        help='브라우저를 열어 GSC 로그인 후 쿠키 저장')
    parser.add_argument('--headless', action='store_true', default=True,
                        help='headless 모드로 실행 (기본값)')
    parser.add_argument('--site', type=str,
                        help='특정 사이트만 실행 (label 지정)')
    parser.add_argument('--no-compare', action='store_true',
                        help='전주 비교 없이 현재 스냅샷만')
    parser.add_argument('--config', type=str, default=default_config,
                        help=f'설정 파일 경로 (기본: {default_config})')
    parser.add_argument('--output-dir', type=str, default=default_output,
                        help=f'출력 디렉토리 (기본: {default_output})')

    args = parser.parse_args()
    asyncio.run(run(args))


if __name__ == '__main__':
    main()
