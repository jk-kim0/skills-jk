#!/usr/bin/env python3.12
"""
GSC 인덱싱 상태 주간 리포트 생성기

GSC "Why pages aren't indexed" 데이터를 Playwright로 스크래핑하여
주간 변화 추적 리포트를 자동 생성합니다.

사용법:
    gsc-index-report --login              # 최초 쿠키 설정 (브라우저 열림)
    gsc-index-report --headless           # headless 모드 실행
    gsc-index-report --site querypie.com  # 특정 사이트만 실행
    gsc-index-report --no-compare         # 비교 없이 현재 스냅샷만
"""

import argparse
import asyncio
import json
import os
import sys
import urllib.parse
from datetime import datetime
from pathlib import Path

try:
    from playwright.async_api import async_playwright
except ImportError:
    print("Playwright가 설치되어 있지 않습니다:")
    print("  pip install playwright && playwright install chromium")
    sys.exit(1)


# --- 설정 ---

COOKIE_PATH = Path(os.environ.get(
    'GSC_COOKIE_PATH',
    os.path.expanduser('~/.config/gsc/browser-cookies.json')
))


# --- 함수 ---

def load_config(config_path: str) -> dict:
    """설정 파일을 로드합니다."""
    path = Path(config_path)
    if not path.exists():
        print(f"오류: 설정 파일을 찾을 수 없습니다: {config_path}")
        sys.exit(1)
    with open(path, encoding='utf-8') as f:
        return json.load(f)


async def login_and_save_cookies():
    """브라우저를 열어 GSC 로그인 후 쿠키를 저장합니다."""
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        context = await browser.new_context()
        page = await context.new_page()
        await page.goto('https://search.google.com/search-console')
        print("브라우저에서 Google 계정으로 로그인하세요.")
        input("로그인 후 Enter를 눌러주세요...")
        cookies = await context.cookies()
        COOKIE_PATH.parent.mkdir(parents=True, exist_ok=True)
        with open(COOKIE_PATH, 'w', encoding='utf-8') as f:
            json.dump(cookies, f, ensure_ascii=False, indent=2)
        print(f"쿠키 저장 완료: {COOKIE_PATH}")
        await browser.close()


async def load_cookies(context):
    """저장된 쿠키를 브라우저 컨텍스트에 로드합니다."""
    if not COOKIE_PATH.exists():
        print(f"오류: 쿠키 파일이 없습니다: {COOKIE_PATH}")
        print("먼저 --login으로 로그인하세요.")
        sys.exit(1)
    with open(COOKIE_PATH, encoding='utf-8') as f:
        cookies = json.load(f)
    await context.add_cookies(cookies)


def build_gsc_page_indexing_url(site_url: str) -> str:
    """GSC Page Indexing 리포트 URL 생성."""
    encoded = urllib.parse.quote(site_url, safe='')
    return f"https://search.google.com/search-console/page-indexing?resource_id={encoded}"


async def scrape_site(page, site_url: str, site_label: str) -> dict:
    """GSC에서 사이트의 인덱싱 상태를 스크래핑합니다."""
    url = build_gsc_page_indexing_url(site_url)
    await page.goto(url, wait_until='networkidle')

    # "Why pages aren't indexed" 테이블 로드 대기
    # GSC는 Angular 기반 SPA — 테이블이 동적으로 렌더링됨
    await page.wait_for_selector('table', timeout=30000)

    # 테이블 행 파싱
    rows = await page.query_selector_all('table tbody tr')
    reasons = []
    for row in rows:
        cells = await row.query_selector_all('td')
        if len(cells) >= 2:
            reason = (await cells[0].inner_text()).strip()
            pages_text = (await cells[-1].inner_text()).strip()
            source = (await cells[1].inner_text()).strip() if len(cells) >= 3 else ''
            validation = (await cells[2].inner_text()).strip() if len(cells) >= 4 else ''
            pages_count = int(pages_text.replace(',', '')) if pages_text.replace(',', '').isdigit() else 0
            reasons.append({
                'reason': reason,
                'source': source,
                'validation': validation,
                'pages': pages_count,
            })

    return {
        'site_url': site_url,
        'label': site_label,
        'reasons': reasons,
        'scraped_at': datetime.now().isoformat(),
    }


async def scrape_site_with_retry(page, site_url: str, site_label: str, max_retries: int = 2) -> dict:
    """재시도 로직이 포함된 스크래핑."""
    for attempt in range(max_retries + 1):
        try:
            return await scrape_site(page, site_url, site_label)
        except Exception as e:
            if attempt == max_retries:
                return {
                    'site_url': site_url,
                    'label': site_label,
                    'error': str(e),
                    'scraped_at': datetime.now().isoformat(),
                }
            print(f"  재시도 {attempt + 1}/{max_retries}: {e}")
            await page.wait_for_timeout(3000)


def save_data(data: dict, output_dir: str) -> Path:
    """스크래핑 데이터를 JSON으로 저장합니다."""
    data_dir = Path(output_dir) / 'data'
    data_dir.mkdir(parents=True, exist_ok=True)
    date_str = datetime.now().strftime('%Y-%m-%d')
    path = data_dir / f'gsc-index-{date_str}.json'
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    return path


def find_previous_data(output_dir: str) -> dict | None:
    """이전 주 데이터를 로드합니다."""
    data_dir = Path(output_dir) / 'data'
    files = sorted(data_dir.glob('gsc-index-*.json'), reverse=True)
    today = datetime.now().strftime('%Y-%m-%d')
    for f in files:
        if today not in f.name:
            with open(f, encoding='utf-8') as fh:
                return json.load(fh)
    return None


def compare_with_previous(current_data: dict, previous_data: dict | None) -> dict:
    """현재 데이터와 이전 데이터를 비교합니다."""
    if previous_data is None:
        return {'has_previous': False}

    comparison = {'has_previous': True, 'sites': {}}
    prev_by_label = {s['label']: s for s in previous_data.get('sites', [])}

    for site in current_data.get('sites', []):
        label = site['label']
        prev_site = prev_by_label.get(label)
        if not prev_site or 'error' in site:
            comparison['sites'][label] = {'new_site': True}
            continue

        prev_reasons = {r['reason']: r['pages'] for r in prev_site.get('reasons', [])}
        curr_reasons = {r['reason']: r['pages'] for r in site.get('reasons', [])}

        changes = {}
        all_reasons = set(prev_reasons) | set(curr_reasons)
        for reason in all_reasons:
            prev_count = prev_reasons.get(reason, 0)
            curr_count = curr_reasons.get(reason, 0)
            diff = curr_count - prev_count
            is_new = reason not in prev_reasons
            is_resolved = reason not in curr_reasons
            changes[reason] = {
                'previous': prev_count,
                'current': curr_count,
                'diff': diff,
                'is_new': is_new,
                'is_resolved': is_resolved,
            }
        comparison['sites'][label] = changes

    return comparison


def generate_report(current_data: dict, comparison: dict, output_dir: str) -> Path:
    """Markdown 리포트를 생성합니다."""
    return Path(output_dir) / 'placeholder.md'


async def run(args):
    """메인 실행 흐름."""
    if args.login:
        await login_and_save_cookies()
        return

    print("스크래핑 기능은 아직 구현되지 않았습니다.")


def main():
    script_dir = Path(__file__).resolve().parent.parent
    default_config = str(script_dir / 'config' / 'gsc-sites.json')
    default_output = str(script_dir / 'reports')

    parser = argparse.ArgumentParser(
        description='GSC 인덱싱 상태 주간 리포트 생성기',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
예시:
  gsc-index-report --login              # 최초 쿠키 설정
  gsc-index-report --headless           # headless 모드 실행
  gsc-index-report --site querypie.com  # 특정 사이트만
  gsc-index-report --no-compare         # 비교 없이 스냅샷만
"""
    )
    parser.add_argument('--login', action='store_true',
                        help='브라우저를 열어 GSC 로그인 후 쿠키 저장')
    parser.add_argument('--headless', action='store_true', default=True,
                        help='headless 모드로 실행 (기본값)')
    parser.add_argument('--site', type=str,
                        help='특정 사이트만 실행 (label 지정)')
    parser.add_argument('--no-compare', action='store_true',
                        help='전주 비교 없이 현재 스냅샷만')
    parser.add_argument('--config', type=str, default=default_config,
                        help=f'설정 파일 경로 (기본: {default_config})')
    parser.add_argument('--output-dir', type=str, default=default_output,
                        help=f'출력 디렉토리 (기본: {default_output})')

    args = parser.parse_args()
    asyncio.run(run(args))


if __name__ == '__main__':
    main()
